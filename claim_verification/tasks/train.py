import torch
from tqdm import tqdm
import torch.optim as optim
import torch.nn as nn

from text_module.text_embedding import TextEmbedding
from load_data.data_loader import Dataset, getDataloader
from models.model import PhobertClassi

def train(model, text_embedding, train_loader, criterion, optimizer, device, epoch):
    model.train()
    avg_loss = 0
    total_true_pred = 0
    total_sample = 0

    for batch in tqdm(train_loader, desc=f"TRAINING EPOCH {epoch}"):
        claim, evidence, label = batch["claim"], batch["evidence"], batch["label"]
        
        #text_embedding
        input_ids = text_embedding(claim, evidence).to(device)

        #forward call
        out, out_soft = model(input_ids)
        loss = criterion(out.to(device), label.to(device))
        avg_loss += loss

        #calc acc
        total_true_pred += (out_soft.argmax(dim=-1).to(device) == label.to(device)).sum()
        total_sample += label.size()[0]

        #backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    return avg_loss/len(train_loader), total_true_pred/total_sample

def eval(model, text_embedding, eval_loader, criterion, device, epoch):
    model.eval()
    avg_loss = 0
    total_true_pred = 0
    total_sample = 0

    with torch.no_grad():
        for batch in tqdm(eval_loader, desc=f"EVALUATING EPOCH {epoch}"):
            claim, evidence, label = batch["claim"], batch["evidence"], batch["label"]

            #text_embedding
            input_ids = text_embedding(claim, evidence).to(device)

            #forward call
            out, out_soft = model(input_ids)
            loss = criterion(out.to(device), label.to(device))
            avg_loss += loss

            #calc acc
            total_true_pred += (out_soft.argmax(dim=-1).to(device) == label.to(device)).sum()
            total_sample += label.size()[0]
    return avg_loss/len(eval_loader), total_true_pred/total_sample

def main(config):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("ðŸ’©ðŸ’©ðŸ’©ðŸ’©ðŸ’©DEVICE LÃ€:", device)

    #khá»Ÿi táº¡o text_embedding
    text_embedding = TextEmbedding(config).to(device)
    print("ðŸ’©ðŸ’©ðŸ’©ðŸ’©ðŸ’©KHá»žI Táº O TEXT EMBEDDING XONG.")

    #load data
    train_dataset = Dataset(config["data_path"]["train"])
    train_loader = getDataloader(train_dataset, config["train"]["batch_size"])
    eval_dataset = Dataset(config["data_path"]["eval"])
    eval_loader = getDataloader(eval_dataset, config["train"]["batch_size"])

    print("ðŸ’©ðŸ’©ðŸ’©ðŸ’©ðŸ’©LOAD DATA THÃ€NH CÃ”NG.")

    #khá»Ÿi táº¡o model
    model = PhobertClassi(config).to(device)
    print("ðŸ’©ðŸ’©ðŸ’©ðŸ’©ðŸ’©KHá»žI Táº O MODEL THÃ€NH CÃ”NG.")

    #cÃ i Ä‘áº·t optim vÃ  criterion
    optimizer = optim.AdamW(model.parameters(), config["train"]["learning_rate"])
    criterion = nn.CrossEntropyLoss().to(device)

    #train
    for epoch in range(config["train"]["num_epochs"]):
        avg_loss, acc = train(model, text_embedding, train_loader, criterion, optimizer, device, epoch + 1)
        avg_loss_eval, acc_eval = eval(model, text_embedding, eval_loader, criterion, device, epoch + 1)
        print("ðŸ’©ðŸ’©ðŸ’©ðŸ’©ðŸ’©-Avg loss: {:.4f} ðŸ’©ðŸ’©ðŸ’©ðŸ’©ðŸ’©-Accuracy: {:.2f}%".format(avg_loss.item(), acc.item()*100))
        print("ðŸ’©ðŸ’©-Avg loss eval: {:.4f} ðŸ’©ðŸ’©-Accuracy eval: {:.2f}%".format(avg_loss_eval.item(), acc_eval.item()*100))

    #save model
    torch.save(model.state_dict(), "/kaggle/working/check_point(not_rename).pth")

